{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 第1章: 基本的なプロンプト構造\n",
        "\n",
        "- [レッスン](#lesson)\n",
        "- [演習](#exercises)\n",
        "- [例のプレイグラウンド](#example-playground)\n",
        "\n",
        "## セットアップ\n",
        "\n",
        "次のセットアップセルを実行して、APIキーをロードし、`get_completion`ヘルパー関数を確立します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install -qU pip\n",
        "%pip install -qUr requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Pythonの組み込み正規表現ライブラリをインポート\n",
        "import re\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import json\n",
        "\n",
        "# utilsパッケージからhintsモジュールをインポート\n",
        "from utils import hints\n",
        "\n",
        "# IPythonストアからMODEL_NAME変数を取得\n",
        "%store -r modelId\n",
        "%store -r region\n",
        "\n",
        "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_completion(prompt, system_prompt=None):\n",
        "    # 推論設定を定義します\n",
        "    inference_config = {\n",
        "        \"temperature\": 0.0,  # 多様な応答を生成するための温度を設定します\n",
        "        \"maxTokens\": 200,  # 生成するトークンの最大数を設定します\n",
        "        \"topP\": 1,  # ニュークリアスamplingのためのtop_p値を設定します\n",
        "    }\n",
        "    # converseメソッドのパラメータを作成します\n",
        "    converse_api_params = {\n",
        "        \"modelId\": modelId,  # 使用するモデルIDを指定します\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],  # ユーザーのプロンプトを提供します\n",
        "        \"inferenceConfig\": inference_config,  # 推論設定を渡します\n",
        "    }\n",
        "    # system_textが提供されているか確認します\n",
        "    if system_prompt:\n",
        "        # system_textが提供されている場合、converse_params辞書にシステムパラメータを追加します\n",
        "        converse_api_params[\"system\"] = [{\"text\": system_prompt}]\n",
        "\n",
        "    # Bedrockクライアントにリクエストを送信して応答を生成します\n",
        "    try:\n",
        "        response = bedrock_client.converse(**converse_api_params)\n",
        "\n",
        "        # 応答から生成されたテキストコンテンツを抽出します\n",
        "        text_content = response['output']['message']['content'][0]['text']\n",
        "\n",
        "        # 生成されたテキストコンテンツを返します\n",
        "        return text_content\n",
        "\n",
        "    except ClientError as err:\n",
        "        message = err.response['Error']['Message']\n",
        "        print(f\"クライアントエラーが発生しました: {message}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## レッスン\n",
        "\n",
        "Amazon Bedrockは、Anthropic Claudeモデルと共に使用できる3つのAPIを提供しています。レガシーの[Text Completions API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html)、[Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html)、および現在の[Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)です。このチュートリアルでは、Converse APIのみを使用します。\n",
        "\n",
        "最低限、Converse APIを使用してClaudeを呼び出すには、以下のパラメータが必要です：\n",
        "- `modelId`: 呼び出すモデルの[APIモデル名](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns)\n",
        "\n",
        "- `messages`: 入力メッセージの配列。Claude 3モデルは、交互に`user`と`assistant`の会話ターンで動作するように訓練されています。新しい`Message`を作成する際には、messagesパラメータで以前の会話ターンを指定し、モデルは次の会話の`Message`を生成します。\n",
        "  - 各入力メッセージは、`role`と`content`を持つオブジェクトでなければなりません。単一の`user`ロールメッセージを指定することも、複数の`user`および`assistant`メッセージを含めることもできます（その場合は交互にする必要があります）。**最初のメッセージは常に`user`ロールを使用しなければなりません。**\n",
        "  \n",
        "  メッセージの内容は、[(ContentBlock)](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ContentBlock.html)の`content`フィールドに格納します。`text`フィールドにテキストを指定するか、モデルがサポートしている場合は、[(ImageBlock)](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ImageBlock.html)の`image`フィールドに画像の生バイトを渡すこともできます。ContentBlockの他のフィールドは[ツール使用](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html)のためのものです。\n",
        "\n",
        "また、以下のようなオプションのパラメータもあります：\n",
        "- `system`: システムプロンプト - これについては後述します。\n",
        "  \n",
        "- `temperature`: Claudeの応答の変動の度合い。このレッスンや演習では、`temperature`を0に設定しています。\n",
        "\n",
        "- `max_tokens`: 停止する前に生成する最大トークン数。この最大値に達する前にClaudeが停止する場合があることに注意してください。このパラメータは、生成するトークンの絶対的な最大数を指定するだけです。さらに、これは*ハード*ストップであり、Claudeが単語の途中や文の途中で生成を停止する原因となる可能性があります。\n",
        "\n",
        "すべてのAPIパラメータの完全なリストについては、[APIドキュメント](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)をご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 例\n",
        "\n",
        "正しくフォーマットされたプロンプトに対するClaudeの応答を見てみましょう。以下の各セルについて、セルを実行してください（`shift+enter`）、するとClaudeの応答がブロックの下に表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# プロンプト\n",
        "PROMPT = \"Hi Claude, how are you?\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# プロンプト\n",
        "PROMPT = \"Can you tell me the color of the ocean?\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# プロンプト\n",
        "PROMPT = \"What year was Celine Dion born in?\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "正しいConverse APIフォーマットを含まないいくつかのプロンプトを見てみましょう。これらの不正なフォーマットのプロンプトに対して、Converse APIはエラーを返します。\n",
        "\n",
        "まず、`messages`配列に`role`および`content`フィールドが欠けているConverse API呼び出しの例があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ⚠️ **警告:** プロンプト内のmessagesパラメータのフォーマットが不正なため、以下のセルはエラーを返します。これは予期される動作です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Claudeの応答を取得\n",
        "inference_config = {\n",
        "    \"temperature\": 0.0, # 多様な応答を生成するための温度を設定\n",
        "    \"maxTokens\": 200 # 生成するトークンの最大数を設定\n",
        "}\n",
        "\n",
        "converse_api_params = {\n",
        "    \"modelId\": modelId,\n",
        "    \"messages\": [{\"text\":\"Hi Claude, how are you?\"}], # ユーザーのプロンプトを提供\n",
        "    \"inferenceConfig\": inference_config # 推論設定を渡す\n",
        "}\n",
        "\n",
        "response = bedrock_client.converse(**converse_api_params)\n",
        "\n",
        "# Claudeの応答を出力\n",
        "print(response['output']['message']['content'][0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "役割の間で`user`と`assistant`を交互に切り替えないプロンプトがあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ⚠️ **警告:** `user`と`assistant`の役割の交代がないため、Claudeはエラーメッセージを返します。これは予期される動作です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "inference_config = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"maxTokens\": 200\n",
        "}\n",
        "# converseメソッドのパラメータを作成する\n",
        "converse_api_params = {\n",
        "    \"modelId\": modelId,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"text\": \"Celine Dionは何年に生まれましたか？\"}]},\n",
        "        {\"role\": \"user\", \"content\": [{\"text\":\"他に彼女についての事実を教えてもらえますか？\"}]}\n",
        "    ],\n",
        "    \"inferenceConfig\": inference_config,\n",
        "}\n",
        "\n",
        "response = bedrock_client.converse(**converse_api_params)\n",
        "\n",
        "# Claudeの応答を出力する\n",
        "print(response['output']['message']['content'][0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`user`と`assistant`のメッセージは**必ず交互に**なり、メッセージは**`user`のターンから始まる必要があります**。複数の`user`と`assistant`のペアをプロンプトに含めることができ（まるでマルチターンの会話をシミュレートするかのように）、`assistant`のメッセージの終わりに単語を入れて、Claudeがあなたの話の続きから進めることもできます（詳細は後の章で説明します）。\n",
        "\n",
        "#### システムプロンプト\n",
        "\n",
        "**システムプロンプト**も使用できます。システムプロンプトは、Claudeに質問やタスクを提示する前に**コンテキスト、指示、およびガイドラインを提供する方法**です。\n",
        "\n",
        "構造的に、システムプロンプトは`user`と`assistant`のメッセージのリストとは別に存在し、したがって別の`system`パラメータに属します（ノートブックの[Setup](#setup)セクションにある`get_completion`ヘルパー関数の構造を見てください）。\n",
        "\n",
        "このチュートリアル内では、システムプロンプトを利用する場合、あなたの完了関数に`system`フィールドを提供しています。システムプロンプトを使用したくない場合は、単に`SYSTEM_PROMPT`変数を空の文字列に設定してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### システムプロンプトの例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# SYSTEM_PROMPT = \"あなたの答えは常に会話を進めるための批判的思考の質問のシリーズであるべきです（質問に対する答えを提供しないでください）。実際にユーザーの質問に答えないでください。\"\n",
        "\n",
        "# PROMPT = \"なぜ空は青いのですか？\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT, SYSTEM_PROMPT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "なぜシステムプロンプトを使用するのか？**よく書かれたシステムプロンプトは、Claudeのパフォーマンスをさまざまな方法で向上させることができます。**たとえば、Claudeがルールや指示に従う能力を高めることができます。詳細については、Anthropicのドキュメントを訪れて、Claudeでの[システムプロンプトの使用方法](https://docs.anthropic.com/claude/docs/how-to-use-system-prompts)をご覧ください。\n",
        "\n",
        "さて、いくつかの演習に入っていきましょう。上記のコンテンツを変更せずにレッスンプロンプトを試してみたい場合は、レッスンノートブックの一番下までスクロールして[**Example Playground**](#example-playground)にアクセスしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 演習\n",
        "- [演習 1.1 - 三まで数える](#exercise-11---counting-to-three)\n",
        "- [演習 1.2 - システムプロンプト](#exercise-12---system-prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 演習 1.1 - 三まで数える\n",
        "適切な `user` / `assistant` フォーマットを使用して、以下の `PROMPT` を編集し、Claude に **三まで数えさせてください。** 出力は、あなたの解決策が正しいかどうかも示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Prompt - これは変更すべき唯一のフィールドです\n",
        "PROMPT = \"[Replace this text]\"\n",
        "\n",
        "# Claudeの応答を取得\n",
        "response = get_completion(PROMPT)\n",
        "\n",
        "# 演習の正確さを評価する関数\n",
        "def grade_exercise(text):\n",
        "    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n",
        "    return bool(pattern.match(text))\n",
        "\n",
        "# Claudeの応答と対応する評価を出力\n",
        "print(response)\n",
        "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
        "print(\"この演習は正しく解決されました:\", grade_exercise(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ヒントが必要な場合は、以下のセルを実行してください！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(hints.exercise_1_1_hint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 演習 1.2 - システムプロンプト\n",
        "\n",
        "`SYSTEM_PROMPT`を修正して、Claudeが3歳の子供のように反応するようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt - this is the only field you should change\n",
        "SYSTEM_PROMPT = \"[このテキストを置き換えて]\"\n",
        "\n",
        "# Prompt\n",
        "PROMPT = \"空はどれくらい大きいですか？\"\n",
        "\n",
        "# Get Claude's response\n",
        "response = get_completion(PROMPT, SYSTEM_PROMPT)\n",
        "\n",
        "# Function to grade exercise correctness\n",
        "def grade_exercise(text):\n",
        "    return bool(re.search(r\"giggles\", text) or re.search(r\"soo\", text))\n",
        "\n",
        "# Print Claude's response and the corresponding grade\n",
        "print(response)\n",
        "print(\"\\n--------------------------- 採点 ---------------------------\")\n",
        "print(\"この演習は正しく解決されました:\", grade_exercise(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ヒントが必要な場合は、以下のセルを実行してください！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(hints.exercise_1_2_hint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### おめでとうございます！\n",
        "\n",
        "ここまでのすべての演習を解決した場合、次の章に進む準備が整いました。楽しいプロンプト作成を！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 例のプレイグラウンド\n",
        "\n",
        "これは、このレッスンで示されたプロンプトの例を自由に実験し、プロンプトを調整してClaudeの応答にどのように影響するかを見るためのエリアです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# プロンプト\n",
        "PROMPT = \"Hi Claude, how are you?\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# プロンプト\n",
        "PROMPT = \"Can you tell me the color of the ocean?\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# プロンプト\n",
        "PROMPT = \"What year was Celine Dion born in?\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Claudeの応答を取得\n",
        "inference_config = {\n",
        "    \"temperature\": 0.0\n",
        "}\n",
        "additional_model_fields = {\n",
        "    \"max_tokens\": 200\n",
        "}\n",
        "\n",
        "converse_api_params = {\n",
        "    \"modelId\": modelId,\n",
        "    \"messages\": [{\"text\":\"Hi Claude, how are you?\"}],\n",
        "    \"inferenceConfig\": inference_config,\n",
        "    \"additionalModelRequestFields\": additional_model_fields\n",
        "}\n",
        "\n",
        "response = bedrock_client.converse(**converse_api_params)\n",
        "\n",
        "# Claudeの応答を出力\n",
        "print(response['output']['message']['content'][0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_config = {\n",
        "    \"temperature\": 0.0\n",
        "}\n",
        "additional_model_fields = {\n",
        "    \"top_p\": 1,\n",
        "    \"max_tokens\": 200\n",
        "}\n",
        "converse_api_params = {\n",
        "    \"modelId\": modelId,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"text\": \"Celine Dionは何年に生まれましたか？\"}]},\n",
        "        {\"role\": \"user\", \"content\": [{\"text\":\"他に彼女についての事実を教えてもらえますか？\"}]}\n",
        "    ],\n",
        "    \"inferenceConfig\": inference_config,\n",
        "    \"additionalModelRequestFields\": additional_model_fields\n",
        "}\n",
        "\n",
        "response = bedrock_client.converse(**converse_api_params)\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(response['output']['message']['content'][0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# SYSTEM_PROMPT = \"あなたの答えは常に会話を進めるための批判的思考の質問のシリーズであるべきです（質問に対する答えを提供しないでください）。実際にユーザーの質問に答えないでください。\"\n",
        "\n",
        "# PROMPT = \"なぜ空は青いのですか？\"\n",
        "\n",
        "# Claudeの応答を出力します\n",
        "print(get_completion(PROMPT, SYSTEM_PROMPT))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}