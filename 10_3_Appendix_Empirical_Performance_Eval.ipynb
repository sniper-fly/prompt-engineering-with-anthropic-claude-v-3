{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AIモデルの評価: コード、ヒューマン、モデルベースの採点\n",
        "\n",
        "このノートブックでは、Claude v3のようなAIモデルの効果を評価するために広く使用されている3つの手法について詳しく見ていきます。\n",
        "\n",
        "1. コードベースの採点\n",
        "2. ヒューマン採点\n",
        "3. モデルベースの採点\n",
        "\n",
        "それぞれのアプローチを例を通じて示し、AIのパフォーマンスを評価する際のそれぞれの利点と制限について検討します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## コードベースの採点例: 感情分析\n",
        "\n",
        "この例では、Claudeの映画レビューの感情をポジティブまたはネガティブに分類する能力を評価します。モデルの出力が期待される感情と一致するかどうかを確認するためにコードを使用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# pythonの組み込み正規表現ライブラリをインポート\n",
        "import re\n",
        "\n",
        "# boto3とjsonをインポート\n",
        "import boto3\n",
        "import json\n",
        "from botocore.exceptions import ClientError\n",
        "# 後で使用するためにモデル名とAWSリージョンを保存\n",
        "\n",
        "%store -r modelId\n",
        "%store -r region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for sentiment analysis\n",
        "def build_input_prompt(review):\n",
        "    user_content = f\"\"\"Classify the sentiment of the following movie review as either 'positive' or 'negative' provide only one of those two choices:\n",
        "    <review>{review}</review>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"review\": \"This movie was amazing! The acting was superb and the plot kept me engaged from start to finish.\",\n",
        "        \"golden_answer\": \"positive\"\n",
        "    },\n",
        "    {\n",
        "        \"review\": \"I was thoroughly disappointed by this film. The pacing was slow and the characters were one-dimensional.\",\n",
        "        \"golden_answer\": \"negative\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Function to get completions from the model\n",
        "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=region)\n",
        "\n",
        "def get_completion(messages, system_prompt=None):\n",
        "    inference_config = {\n",
        "        \"temperature\": 0.5,\n",
        "        \"maxTokens\": 200\n",
        "    }\n",
        "    additional_model_fields = {\n",
        "        \"top_p\": 1\n",
        "    }\n",
        "    converse_api_params = {\n",
        "        \"modelId\": modelId,\n",
        "        \"messages\": messages,\n",
        "        \"inferenceConfig\": inference_config,\n",
        "        \"additionalModelRequestFields\": additional_model_fields\n",
        "    }\n",
        "    if system_prompt:\n",
        "        converse_api_params[\"system\"] = [{\"text\": system_prompt}]\n",
        "    try:\n",
        "        response = bedrock_client.converse(**converse_api_params)\n",
        "        text_content = response['output']['message']['content'][0]['text']\n",
        "        return text_content\n",
        "\n",
        "    except ClientError as err:\n",
        "        message = err.response['Error']['Message']\n",
        "        print(f\"A client error occured: {message}\")\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"review\"])) for item in eval]\n",
        "\n",
        "# Print the outputs and golden answers\n",
        "for output, question in zip(outputs, eval):\n",
        "    print(f\"Review: {question['review']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")\n",
        "\n",
        "# Function to grade the completions\n",
        "def grade_completion(output, golden_answer):\n",
        "    return output.lower() == golden_answer.lower()\n",
        "\n",
        "# Grade the completions and print the accuracy\n",
        "grades = [grade_completion(output, item[\"golden_answer\"]) for output, item in zip(outputs, eval)]\n",
        "print(f\"Accuracy: {sum(grades) / len(grades) * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 人間の採点例：エッセイのスコアリング\n",
        "\n",
        "エッセイのスコアリングのようなタスクは、コードだけで評価するのが難しいです。この場合、モデルの出力を評価するための人間の採点者向けのガイドラインを提供できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for essay generation\n",
        "def build_input_prompt(topic):\n",
        "    user_content = f\"\"\"Write a short essay discussing the following topic:\n",
        "    <topic>{topic}</topic>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"topic\": \"The importance of education in personal development and societal progress\",\n",
        "        \"golden_answer\": \"A high-scoring essay should have a clear thesis, well-structured paragraphs, and persuasive examples discussing how education contributes to individual growth and broader societal advancement.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"topic\"])) for item in eval]\n",
        "\n",
        "# Print the outputs and golden answers\n",
        "for output, item in zip(outputs, eval):\n",
        "    print(f\"Topic: {item['topic']}\\n\\nGrading Rubric:\\n {item['golden_answer']}\\n\\nModel Output:\\n{output}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## モデルベースの採点例\n",
        "\n",
        "私たちは、モデルの応答と採点基準を提供することで、Claudeが自分自身の出力を採点することができます。これにより、通常は人間の判断を必要とするタスクの評価を自動化することが可能になります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 例 1: 要約\n",
        "\n",
        "この例では、Claudeを使用して生成した要約の質を評価します。これは、モデルが長いテキストから重要な情報を簡潔かつ正確に捉える能力を評価する必要があるときに役立ちます。カバーすべき重要なポイントを概説したルーブリックを提供することで、採点プロセスを自動化し、要約タスクにおけるモデルのパフォーマンスを迅速に評価できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for summarization\n",
        "def build_input_prompt(text):\n",
        "    user_content = f\"\"\"Please summarize the main points of the following text:\n",
        "    <text>{text}</text>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Function to build the grader prompt for assessing summary quality\n",
        "def build_grader_prompt(output, rubric):\n",
        "    user_content = f\"\"\"Assess the quality of the following summary based on this rubric:\n",
        "    <rubric>{rubric}</rubric>\n",
        "    <summary>{output}</summary>\n",
        "    Provide a score from 1-5, where 1 is poor and 5 is excellent.\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"text\": \"The Magna Carta, signed in 1215, was a pivotal document in English history. It limited the powers of the monarchy and established the principle that everyone, including the king, was subject to the law. This laid the foundation for constitutional governance and the rule of law in England and influenced legal systems worldwide.\",\n",
        "        \"golden_answer\": \"A high-quality summary should concisely capture the key points: 1) The Magna Carta's significance in English history, 2) Its role in limiting monarchical power, 3) Establishing the principle of rule of law, and 4) Its influence on legal systems around the world.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"text\"])) for item in eval]\n",
        "\n",
        "# Grade the completions\n",
        "grades = [get_completion(build_grader_prompt(output, item[\"golden_answer\"])) for output, item in zip(outputs, eval)]\n",
        "\n",
        "# Print the summary quality score\n",
        "print(f\"Summary quality score: {grades[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 例2: ファクトチェック\n",
        "\n",
        "この例では、Claudeを使用して主張をファクトチェックし、そのファクトチェックの正確性を評価します。これは、モデルが正確な情報と不正確な情報を区別する能力を評価する必要があるときに役立ちます。正しいファクトチェックでカバーすべき重要なポイントを概説したルーブリックを提供することで、採点プロセスを自動化し、ファクトチェックタスクにおけるモデルのパフォーマンスを迅速に評価できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for fact-checking\n",
        "def build_input_prompt(claim):\n",
        "    user_content = f\"\"\"Determine if the following claim is true or false:\n",
        "    <claim>{claim}</claim>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Function to build the grader prompt for assessing fact-check accuracy\n",
        "def build_grader_prompt(output, rubric):\n",
        "    user_content = f\"\"\"Based on the following rubric, assess whether the fact-check is correct:\n",
        "    <rubric>{rubric}</rubric>\n",
        "    <fact-check>{output}</fact-check>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"claim\": \"The Great Wall of China is visible from space.\",\n",
        "        \"golden_answer\": \"A correct fact-check should state that this claim is false. While the Great Wall is an impressive structure, it is not visible from space with the naked eye.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"claim\"])) for item in eval]\n",
        "\n",
        "grades = []\n",
        "for output, item in zip(outputs, eval):\n",
        "    # 主張、ファクトチェック、およびルブリックを出力します\n",
        "    print(f\"Claim: {item['claim']}\\n\")\n",
        "    print(f\"Fact-check: {output}]\\n\")\n",
        "    print(f\"Rubric: {item['golden_answer']}\\n\")\n",
        "    \n",
        "    # ファクトチェックを評価します\n",
        "    grader_prompt = build_grader_prompt(output, item[\"golden_answer\"])\n",
        "    grade = get_completion(grader_prompt)\n",
        "    grades.append(\"correct\" in grade.lower())\n",
        "\n",
        "# ファクトチェックの正確性を出力します\n",
        "accuracy = sum(grades) / len(grades)\n",
        "print(f\"Fact-checking accuracy: {accuracy * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 例 3: トーン分析\n",
        "\n",
        "この例では、Claudeを使用して与えられたテキストのトーンを分析し、その分析の正確性を評価します。これは、モデルがテキストの中で表現された感情的な内容や態度を特定し解釈する能力を評価する必要があるときに役立ちます。特定すべきトーンの重要な側面を概説したルーブリックを提供することで、採点プロセスを自動化し、トーン分析タスクにおけるモデルのパフォーマンスを迅速に評価できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for tone analysis\n",
        "def build_input_prompt(text):\n",
        "    user_content = f\"\"\"以下のテキストのトーンを分析してください:\n",
        "    <text>{text}</text>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Function to build the grader prompt for assessing tone analysis accuracy\n",
        "def build_grader_prompt(output, rubric):\n",
        "    user_content = f\"\"\"このルブリックに基づいて、以下のトーン分析の正確性を評価してください:\n",
        "    <rubric>{rubric}</rubric>\n",
        "    <tone-analysis>{output}</tone-analysis>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": [{\"text\": user_content}]}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"text\": \"イベントが直前にキャンセルされたなんて信じられません。これは完全に受け入れられず、非専門的です！\",\n",
        "        \"golden_answer\": \"トーン分析は、テキストがフラストレーション、怒り、失望を表現していることを特定する必要があります。'信じられない'、'直前'、'受け入れられない'、'非専門的'のようなキーワードは、強い否定的感情を示しています。\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"text\"])) for item in eval]\n",
        "\n",
        "# Grade the completions\n",
        "grades = [get_completion(build_grader_prompt(output, item[\"golden_answer\"])) for output, item in zip(outputs, eval)]\n",
        "\n",
        "# Print the tone analysis quality\n",
        "print(f\"トーン分析の質: {grades[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "これらの例は、`Claude`のようなAIモデルをさまざまなタスクで評価するために、コードベース、人間、モデルベースの採点がどのように使用できるかを示しています。評価方法の選択は、タスクの性質と利用可能なリソースに依存します。モデルベースの採点は、人間の判断を必要とする複雑なタスクの評価を自動化するための有望なアプローチを提供します。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}